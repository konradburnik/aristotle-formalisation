/-
This file was generated by Aristotle.

Lean version: leanprover/lean4:v4.24.0
Mathlib version: f897ebcf72cd16f89ab4577d0c826cd14afaafc7
This project request had uuid: da91f201-44ef-41fa-8aa3-59b113cd3306

To cite Aristotle, tag @Aristotle-Harmonic on GitHub PRs/issues, and add as co-author to commits:
Co-authored-by: Aristotle (Harmonic) <aristotle-harmonic@harmonic.fun>

Aristotle's budget for this request has been reached.
If there is partial progress, it will appear in this file.
If you would like to continue working off of this partial progress,
please submit the same prompt, and add this .lean file in "Optional: Attach a Lean file as context" field.
-/

import Mathlib

set_option linter.mathlibStandardSet false

open scoped BigOperators
open scoped Real
open scoped Nat
open scoped Classical
open scoped Pointwise

set_option maxHeartbeats 0
set_option maxRecDepth 4000
set_option synthInstance.maxHeartbeats 20000
set_option synthInstance.maxSize 128

set_option relaxedAutoImplicit false
set_option autoImplicit false

noncomputable section

/-
The set of functions representable by a neural network with one hidden layer and activation function σ.
-/
open Set Filter Metric Topology

/-- The set of functions from `ℝ^n` to `ℝ` that can be represented by a single hidden layer
neural network with activation function `σ`. -/
def NeuralNetwork (n : ℕ) (σ : ℝ → ℝ) : Set (EuclideanSpace ℝ (Fin n) → ℝ) :=
  Submodule.span ℝ {f | ∃ (w : EuclideanSpace ℝ (Fin n)) (b : ℝ), f = fun x => σ (inner ℝ w x + b)}

/-
A function σ is discriminatory if the only continuous linear functional on C(K, R) that vanishes on all functions of the form x ↦ σ(w • x + b) is the zero functional.
-/
open Set Filter Metric Topology

/-- A function `σ` is discriminatory if the only continuous linear functional on `C(K, ℝ)`
that vanishes on all functions of the form `x ↦ σ(w • x + b)` is the zero functional. -/
def Discriminatory (n : ℕ) (σ : ℝ → ℝ) (hσ : Continuous σ) : Prop :=
  ∀ (K : Set (EuclideanSpace ℝ (Fin n))) (hK : IsCompact K),
  ∀ (L : C(K, ℝ) →L[ℝ] ℝ),
  (∀ (w : EuclideanSpace ℝ (Fin n)) (b : ℝ),
    let f : C(K, ℝ) := ⟨fun x => σ (inner ℝ w (x : EuclideanSpace ℝ (Fin n)) + b),
      hσ.comp (Continuous.add (Continuous.inner continuous_const continuous_subtype_val) continuous_const)⟩
    L f = 0) →
  L = 0

/-
Every function in a neural network with a continuous activation function is continuous.
-/
open Set Filter Metric Topology

/-- Every function in the neural network is continuous. -/
theorem continuous_of_mem_neural_network (n : ℕ) (σ : ℝ → ℝ) (hσ : Continuous σ)
    (g : EuclideanSpace ℝ (Fin n) → ℝ) (hg : g ∈ NeuralNetwork n σ) :
    Continuous g := by
  refine' continuous_iff_continuousAt.mpr fun x => _;
  have hg_span : (g ∈ Submodule.span ℝ {f : EuclideanSpace ℝ (Fin n) → ℝ | ∃ w : EuclideanSpace ℝ (Fin n), ∃ b : ℝ, f = fun x => σ (inner ℝ w x + b)}) := by
    exact hg;
  refine' ( Submodule.span_induction _ _ _ _ hg_span );
  · rintro _ ⟨ w, b, rfl ⟩ ; exact Continuous.continuousAt ( hσ.comp ( Continuous.add ( continuous_const.inner continuous_id' ) continuous_const ) );
  · exact continuousAt_const;
  · exact fun f g hf hg hf' hg' => ContinuousAt.add hf' hg';
  · exact fun a f hf hf' => ContinuousAt.mul continuousAt_const hf'

/-
If the only continuous linear functional vanishing on a subspace is zero, then the subspace is dense.
-/
open Set Filter Metric Topology

/-- If the only continuous linear functional vanishing on a subspace is zero, then the subspace is dense. -/
theorem dense_of_forall_dual_eq_zero {E : Type*} [NormedAddCommGroup E] [NormedSpace ℝ E]
    (S : Submodule ℝ E)
    (h : ∀ (f : E →L[ℝ] ℝ), (∀ x ∈ S, f x = 0) → f = 0) :
    Dense (S : Set E) := by
  by_contra h_not_dense;
  -- Since $S$ is not dense, there exists $x₀ \notin \overline{S}$.
  obtain ⟨x₀, hx₀⟩ : ∃ x₀ : E, x₀ ∉ closure (S : Set E) := by
    simpa [ Dense ] using h_not_dense;
  -- By the geometric Hahn-Banach theorem, there exists a continuous linear functional $f$ and a constant $c$ such that $f(x₀) > c$ and $f(x) \leq c$ for all $x \in \overline{S}$.
  obtain ⟨f, c, hf⟩ : ∃ (f : E →L[ℝ] ℝ), ∃ (c : ℝ), f x₀ > c ∧ ∀ x ∈ closure (S : Set E), f x ≤ c := by
    have := @geometric_hahn_banach_closed_point E;
    specialize this ( show Convex ℝ ( closure ( S : Set E ) ) from ?_ ) ( show IsClosed ( closure ( S : Set E ) ) from ?_ ) hx₀;
    · exact S.convex.closure;
    · exact isClosed_closure;
    · exact ⟨ this.choose, this.choose_spec.choose, this.choose_spec.choose_spec.2, fun x hx => le_of_lt ( this.choose_spec.choose_spec.1 x hx ) ⟩;
  -- Since $S$ is a subspace, $f$ must be zero on $S$.
  have hf_zero : ∀ x ∈ S, f x = 0 := by
    intro x hx
    by_cases hfx : f x = 0;
    · exact hfx;
    · -- Since $f$ is continuous and $f(x) \neq 0$, there exists a scalar $t$ such that $f(tx) > c$.
      obtain ⟨t, ht⟩ : ∃ t : ℝ, f (t • x) > c := by
        norm_num +zetaDelta at *;
        exact ⟨ ( c + 1 ) / f x, by rw [ div_mul_cancel₀ _ hfx ] ; linarith ⟩;
      exact False.elim ( ht.not_le ( hf.2 _ ( subset_closure ( S.smul_mem t hx ) ) ) );
  exact absurd ( h f hf_zero ) ( by rintro rfl; exact not_lt_of_ge ( hf.2 0 ( subset_closure S.zero_mem ) ) ( by simpa using hf.1 ) )

/-
The set of restrictions of neural networks to a compact set is dense in the space of continuous functions if the activation function is discriminatory.
-/
open Set Filter Metric Topology

/-- The submodule of `C(K, ℝ)` generated by the restrictions of neural networks. -/
def NeuralNetworkRestriction (n : ℕ) (σ : ℝ → ℝ) (hσ : Continuous σ)
    (K : Set (EuclideanSpace ℝ (Fin n))) : Submodule ℝ C(K, ℝ) :=
  Submodule.span ℝ {f : C(K, ℝ) | ∃ (w : EuclideanSpace ℝ (Fin n)) (b : ℝ),
    f = ⟨fun x => σ (inner ℝ w (x : EuclideanSpace ℝ (Fin n)) + b),
         hσ.comp (Continuous.add (Continuous.inner continuous_const continuous_subtype_val) continuous_const)⟩}

/-- If `σ` is discriminatory, then the restriction submodule is dense. -/
theorem dense_restriction_of_discriminatory (n : ℕ) (σ : ℝ → ℝ) (hσ_cont : Continuous σ)
    (hσ_disc : Discriminatory n σ hσ_cont)
    (K : Set (EuclideanSpace ℝ (Fin n))) (hK : IsCompact K) :
    Dense (NeuralNetworkRestriction n σ hσ_cont K : Set C(K, ℝ)) := by
      -- We will show that if a continuous linear functional vanishes on the restrictions of neural networks to K, then it must be zero.
      have h_dual_zero : ∀ (L : C(K, ℝ) →L[ℝ] ℝ), (∀ (w : EuclideanSpace ℝ (Fin n)) (b : ℝ),
          L ⟨fun x => σ (inner ℝ w x + b), hσ_cont.comp (Continuous.add (Continuous.inner continuous_const continuous_subtype_val) continuous_const)⟩ = 0) → L = 0 := by
            exact?;
      by_contra h_contra;
      -- Apply the Hahn-Banach theorem to obtain a non-zero continuous linear functional that vanishes on the submodule.
      obtain ⟨L, hL_nonzero, hL_zero⟩ : ∃ L : C(K, ℝ) →L[ℝ] ℝ, L ≠ 0 ∧ ∀ x ∈ NeuralNetworkRestriction n σ hσ_cont K, L x = 0 := by
        have h_hahn_banach : ∀ (S : Submodule ℝ (C(K, ℝ))), ¬Dense (S : Set (C(K, ℝ))) → ∃ L : C(K, ℝ) →L[ℝ] ℝ, L ≠ 0 ∧ ∀ x ∈ S, L x = 0 := by
          intros S hS_not_dense
          obtain ⟨x, hx⟩ : ∃ x : C(K, ℝ), x ∉ closure (S : Set (C(K, ℝ))) := by
            simpa [ Dense ] using hS_not_dense;
          have := @geometric_hahn_banach_closed_point ( C(K, ℝ) );
          specialize this ( show Convex ℝ ( closure ( S : Set ( C(K, ℝ) ) ) ) from ?_ ) ( show IsClosed ( closure ( S : Set ( C(K, ℝ) ) ) ) from isClosed_closure ) hx;
          · exact S.convex.closure;
          · obtain ⟨ f, u, hf, hu ⟩ := this;
            refine' ⟨ f, _, _ ⟩;
            · rintro rfl; norm_num at hu;
              exact hu.not_le ( le_of_lt ( hf 0 ( subset_closure ( S.zero_mem ) ) ) );
            · intro y hy;
              have h_zero : ∀ t : ℝ, f (t • y) < u := by
                exact fun t => hf _ <| subset_closure <| S.smul_mem t hy;
              simp +zetaDelta at *;
              exact le_antisymm ( le_of_not_gt fun h => by have := h_zero ( u / f y ) ; rw [ div_mul_cancel₀ _ h.ne' ] at this; linarith ) ( le_of_not_gt fun h => by have := h_zero ( u / f y ) ; rw [ div_mul_cancel₀ _ h.ne ] at this; linarith );
        exact h_hahn_banach _ h_contra;
      exact hL_nonzero <| h_dual_zero L <| by rintro w b; exact hL_zero _ <| Submodule.subset_span ⟨ w, b, rfl ⟩ ;

#check continuous_sigmoid

open Set Filter Metric Topology

variable (n : ℕ) (K : Set (EuclideanSpace ℝ (Fin n))) [CompactSpace K]

#synth MetricSpace C(K, ℝ)
#check continuous_of_mem_neural_network

/-
Every function in the restriction submodule can be lifted to a global neural network function.
-/
open Set Filter Metric Topology

/-- Every function in the restriction submodule can be lifted to a global neural network function. -/
lemma exists_global_of_mem_restriction (n : ℕ) (σ : ℝ → ℝ) (hσ : Continuous σ)
    (K : Set (EuclideanSpace ℝ (Fin n)))
    (g : C(K, ℝ)) (hg : g ∈ NeuralNetworkRestriction n σ hσ K) :
    ∃ G ∈ NeuralNetwork n σ, ∀ x : K, G x = g x := by
      norm_num +zetaDelta at *;
      refine' Submodule.span_induction _ _ _ _ hg;
      · rintro f ⟨ w, b, rfl ⟩;
        exact ⟨ _, Submodule.subset_span ⟨ w, b, rfl ⟩, fun x hx => rfl ⟩;
      · exact ⟨ 0, Submodule.zero_mem _, fun _ _ => rfl ⟩;
      · rintro x y hx hy ⟨ G₁, hG₁, hG₁' ⟩ ⟨ G₂, hG₂, hG₂' ⟩ ; exact ⟨ G₁ + G₂, Submodule.add_mem _ hG₁ hG₂, fun a ha => by simp +decide [ hG₁' a ha, hG₂' a ha ] ⟩ ;
      · rintro a x hx ⟨ G, hG, hG' ⟩ ; use a • G; aesop;